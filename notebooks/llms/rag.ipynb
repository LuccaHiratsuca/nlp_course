{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537ca283",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "396c58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49007fc9",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5404e7",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e0c74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../classic_nlp/01-regex.ipynb'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_classic_1 = '../classic_nlp/01-regex.ipynb'\n",
    "notebook_classic_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55bcde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../classic_nlp/01-regex.ipynb', 'r', encoding='utf-8') as file:\n",
    "\tnotebook_classic_1_json = json.load(file)\n",
    "# notebook_classic_1_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f798e9",
   "metadata": {},
   "source": [
    "1. How can we find a cell within the notebook?\n",
    "- We can find a cell within the notebook by using the `find` command in the command palette (Cmd + Shift + P) and typing \"find\". This will allow us to search for specific text within the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f50570",
   "metadata": {},
   "source": [
    "2. How can we known if the cell contains Python code or Markdown annotations?\n",
    "\n",
    "- `cell_type` is the attribute that tells us if the cell is a code cell or a markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8418e06",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f411b9a8",
   "metadata": {},
   "source": [
    "The simplest way to search for text is using keywords. Improve your code so that it:\n",
    "\n",
    "1. Collects a keyword from the user, and\n",
    "2. Indicates all notebooks/cells that contain that keyword.\n",
    "\n",
    "Reflect: what is the best way to present these results to the user?\n",
    "\n",
    "- The best way to present the results to the user is to display the notebook name and the cell number where the keyword was found. This way, the user can easily navigate to the specific cell in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f98a8074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword 'fascinating' found in 2 cell(s) in notebook '{'cells': [{'cell_type': 'markdown', 'metadata': {}, 'source': ['# Regular expressions: finding text within text\\n', '\\n']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['## Exercise 1\\n', '\\n', 'There are many situations that call for detecting particular words in a text. For example, we could count how many times the word \"are\" appears in a text:']}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {}, 'outputs': [], 'source': ['text = \"\"\"\\n', 'Llamas are fascinating animals that are found in the Andes Mountains.\\n', 'Yoda says that cute animals they are.\\n', 'Are llamas cute? Yes they are!\\n', 'A llama never leaves its herd and will protect it with its life.\\n', 'They are known for their long necks and thick fur.\\n', 'Llamas are used as pack animals by indigenous people because they are strong and can carry heavy loads.\\n', 'They are also very social animals and are often seen in groups.\\n', 'Llamas ARE herbivores, which means they are only eating plants.\\n', 'They are also known for their gentle and calm nature, which makes them popular in petting zoos.\\n', 'Overall, llamas are remarkable creatures that are a joy to observe and are important to the cultures where they are found.\\n', '\"\"\"']}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['The word \"are\" appears 15 times in this text.\\n']}], 'source': ['def find_words(word_to_find, text):\\n', '    words = text.lower().split()\\n', '    matching_words = []\\n', '    for w in words:\\n', '        if w == word_to_find:\\n', '            matching_words.append(w)\\n', '    return len(matching_words)\\n', '\\n', 'word_to_find = \"are\"\\n', 'count = find_words(word_to_find, text)\\n', 'print(f\\'The word \"are\" appears {count} times in this text.\\')']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['This is a somewhat naive approach. It tends to miss variations of a word. For example, the word \"are.\" in the end of the second line is not counted in this approach\\n', '\\n', 'Spot the ocurrences of the word \"are\" that are not caught by this code.\\n', '\\n', 'Hint: how does string equality (`s1==s2`) work with uppercase/lowercase or punctuation?']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['## Exercise 2\\n', '\\n', \"If you check the documentation for Python's string library, you will find many methods that can help you make a better detector for words.\\n\", '\\n', 'Use them to rewrite the method `find_words_less_naive` below so that it:\\n', '\\n', '* Works when words have punctuation (that is, ignores punctuation)\\n', '* Ignores case (uppercase/lowercase),\\n', '* Does not count a word if it appears in a plural (ex: says does not count as a say).']}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {}, 'outputs': [], 'source': ['import re\\n']}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['The word \"are\" appears 0 times in this text.\\n']}], 'source': ['import re\\n', '\\n', 'def find_words_less_naive(word_to_find, text):\\n', \"    words = re.sub(r'[^\\\\w\\\\s]', '', text).lower().split()\\n\", '    words_not_plural = \"\".join([w[:-1] if w.endswith(\\'s\\') else w for w in words])\\n', '    matching_words = []\\n', '    for w in words_not_plural:\\n', '        if w == word_to_find: \\n', '            matching_words.append(w)\\n', '    return len(matching_words)\\n', '\\n', '\\n', 'word_to_find = \"fascinating\"\\n', 'count = find_words_less_naive(word_to_find, text)\\n', 'print(f\\'The word \"are\" appears {count} times in this text.\\')']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['## Exercise 3\\n', '\\n', 'You may have noticed that the words \"llama\" and \"llamas\" both start with \"llama\".\\n', '\\n', 'If we wanted to make a function searching for either llama our llamas, then we could phrase it somewhat like:\\n', '\\n', '**The sequence `llama` followed by an optional `s`.**\\n', '\\n', 'This type of pattern is called a regular expression.\\n', '\\n', 'A regular expression is a string that contains:\\n', '\\n', '*Usual literal characters, like a, b, or 7\\n', '*Special markers, which present special behaviors.\\n', '\\n', 'We use regular expressions to search for these patterns within strings. It could be seen as a more advanced way of using string.find. Actually, searching for a regular expression without any special character would lead to the same behavior as using string.find.\\n', '\\n', 'Special characters, however, give us some interesting ideas.\\n', '\\n', 'The `?` character, for example, means that the previous character is optional, that is: `llamas?` matches both `llama` and `llamas`.\\n', '\\n', 'aa?bb?cc?\\n', '\\n', 'Which of the strings below would match `aa?bb?cc?`\\n', '\\n', '1. cba\\n', '1. abbcc\\n', '1. abbc\\n', '1. abcc\\n', '1. aa\\n', '1. aabbc\\n', '1. abc\\n', '1. aabc\\n', '1. bcc\\n', '1. abb\\n', '1. aabcc\\n', '1. ac\\n', '1. ccbbaa\\n']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['## Exercise 4\\n', '\\n', 'Now we know how to use sets and questions marks.\\n', '\\n', 'Two other symbols that will be useful are:\\n', '\\n', '* `+` indicates one or more ocurrences of the previous symbol\\n', '* `*` indicates zero or more ocurrences of the previous symbol\\n', '\\n', 'Now, match the regular expressions below with their corresponding strings:\\n', '\\n', '| Strings | Expressions | \\n', '| --- | --- | \\n', '| BAbaaaccc | `[aA][bB]c*` |\\n', '| Bccc | `[aA]?[bB]ccc` |\\n', '| ab | `[aA][bB]c+` |\\n', '| ABccccc | `[aAbB]*c+` |\\n']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['### Answear\\n', '\\n', 'BAbaaaccc: 4\\n', '\\n', 'Bccc: 2\\n', '\\n', 'ab: 1\\n', '\\n', 'ABccccc: 3']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['## Exercise 5\\n', '\\n', 'There are some special sets we should be aware of:\\n', '\\n', 'the `[A-Z]` denotes a range. In this case, the set would contain all uppercase ascii characters from A to Z.\\n', 'Ranges can be combined, like `[A-Za-z]` for all ascii characters\\n', 'Modern regular expressions have sets such as `\\\\w` to denote any character that can form a word, or `\\\\d` for any digit.\\n', 'Check the documentation at https://docs.python.org/3/library/re.html to see more symbols!\\n', '\\n', 'Match the strings below with their corresponding expressions:\\n', '\\n', '| Strings | Expressions | \\n', '| --- | --- | \\n', '| SnakeCaseVariableNames123 | `\\\\w+.com` |\\n', '| nlp.com | `\\\\w+` |\\n', '| 123432412394 | `\\\\w\\\\w\\\\w+[.]com` |\\n', '| Brown | `[A-Za-z][A-Za-z0-9]+` |\\n', '| n!com | `\\\\d+` |\\n']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['## Answer\\n', '\\n', '1: 4\\n', '\\n', '2: 3\\n', '\\n', '3: 5\\n', '\\n', '4: 2\\n', '\\n', '5: 1']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['## Exercise 6 - Crawling the web for information!\\n', '\\n', 'Together with the notebooks from this course, you will find an app that can be called by command line using:\\n', '\\n', '`nlp_course-cli [command] [parameters]`\\n', '\\n', 'One of the available commands is `crawl`. It crawls the web finding more and more links.\\n', '\\n', 'If you start in `main.py`, you will find that the `crawl` command returns information found in each page.\\n', '\\n', 'Modify the code given as example so that the returned info contains more useful elements. Make your system retrieve:\\n', '\\n', '1. hashtags (like `#imcrawling`)\\n', '1. e-mail addresses (plus: can you find spoofed emails like `user [at] server [dot] com`?)\\n', '1. Social network handles (`@userhandle`)\\n', '1. telephone numbers\\n', '1. ZIP codes (In Brazil: CEP numbers)\\n']}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {}, 'outputs': [], 'source': ['import re ']}, {'cell_type': 'code', 'execution_count': 63, 'metadata': {}, 'outputs': [], 'source': ['# Examples:\\n', '\\n', '# Hashtags\\n', 'hashtags = \"ola #imcrawling tudo bem?\"\\n', '\\n', '# Email Addresses\\n', 'emails = \"luccahiratsuca@gmail.com\"\\n', '\\n', '# Social Network Handles\\n', 'social_networks = \"twitter.com/luccahiratsuca teste\"\\n', '\\n', '# Phone Numbers\\n', 'phone_numbers = \"99174-2424\"\\n', '\\n', '# Zip Codes\\n', 'zip_codes = \"12345-123\"\\n']}, {'cell_type': 'code', 'execution_count': 64, 'metadata': {}, 'outputs': [], 'source': [\"HASHTAG_REGEX = r'\\\\#{1}[\\\\w]+'\\n\", '\\n', 'EMAIL_REGEX = r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+\"\\n', '\\n', \"SOCIAL_NETWORKS_REGEX = r'\\\\/[\\\\w]+'\\n\", '\\n', \"PHONE_NUMBERS_REGEX = r'^\\\\d{5}-\\\\d{4}$'\\n\", '\\n', \"ZIP_CODE_REGEX = r'\\\\d{5}-\\\\d{3}'\"]}, {'cell_type': 'code', 'execution_count': 65, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': [\"<re.Match object; span=(0, 24), match='luccahiratsuca@gmail.com'>\\n\", \"['#imcrawling']\\n\", \"['/luccahiratsuca']\\n\", \"['99174-2424']\\n\", \"['12345-123']\\n\"]}], 'source': ['print(re.match(EMAIL_REGEX, emails))\\n', '\\n', 'print(re.findall(HASHTAG_REGEX, hashtags))\\n', '\\n', 'print(re.findall(SOCIAL_NETWORKS_REGEX, social_networks))\\n', '\\n', 'print(re.findall(PHONE_NUMBERS_REGEX, phone_numbers))\\n', '\\n', 'print(re.findall(ZIP_CODE_REGEX, zip_codes))\\n']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['### Playing with regex:\\n']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['a) Extracting number from a string mixed']}, {'cell_type': 'code', 'execution_count': 62, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': ['99174-2424\\n']}], 'source': ['phone_numbers = \"99174-teste2424\"\\n', '\\n', \"PHONE_NUMBERS_REGEX = r'(\\\\d{5})-\\\\D*(\\\\d{4})'\\n\", '\\n', 'match = re.search(PHONE_NUMBERS_REGEX, phone_numbers)\\n', '\\n', 'if match:\\n', '    phone_number = f\"{match.group(1)}-{match.group(2)}\"\\n', '    print(phone_number) \\n']}, {'cell_type': 'markdown', 'metadata': {}, 'source': ['### Understanding `\\\\b` in regex']}, {'cell_type': 'code', 'execution_count': 67, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': [\"['cats']\\n\"]}], 'source': ['import re\\n', '\\n', 'text = \"I love cats, but I also love my cat.\"\\n', '\\n', \"matches = re.findall(r'\\\\bcat\\\\w+', text)\\n\", \"print(matches)  # Output: ['cat', 'cat']\"]}, {'cell_type': 'code', 'execution_count': 68, 'metadata': {}, 'outputs': [{'name': 'stdout', 'output_type': 'stream', 'text': [\"['cats', 'cat']\\n\", \"['cat', 'cats', 'cat']\\n\"]}], 'source': ['text = \"123cat and cats and cat.\"\\n', '\\n', \"matches1 = re.findall(r'\\\\bcat\\\\w*', text)  # Com \\\\b\\n\", \"matches2 = re.findall(r'cat\\\\w*', text)   # Sem \\\\b\\n\", '\\n', \"print(matches1)  # Output: ['cats', 'cat']\\n\", \"print(matches2)  # Output: ['cat', 'cats', 'cat']\"]}], 'metadata': {'kernelspec': {'display_name': 'env', 'language': 'python', 'name': 'python3'}, 'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3}, 'file_extension': '.py', 'mimetype': 'text/x-python', 'name': 'python', 'nbconvert_exporter': 'python', 'pygments_lexer': 'ipython3', 'version': '3.10.10'}}, 'nbformat': 4, 'nbformat_minor': 2}':\n",
      "\n",
      "------------------------------\n",
      "Cell 1:\n",
      "Cell type: code\n",
      "Source:\n",
      " text = \"\"\"\n",
      "Llamas are fascinating animals that are found in the Andes Mountains.\n",
      "Yoda says that cute animals they are.\n",
      "Are llamas cute? Yes they are!\n",
      "A llama never leaves its herd and will protect it with its life.\n",
      "They are known for their long necks and thick fur.\n",
      "Llamas are used as pack animals by indigenous people because they are strong and can carry heavy loads.\n",
      "They are also very social animals and are often seen in groups.\n",
      "Llamas ARE herbivores, which means they are only eating plants.\n",
      "They are also known for their gentle and calm nature, which makes them popular in petting zoos.\n",
      "Overall, llamas are remarkable creatures that are a joy to observe and are important to the cultures where they are found.\n",
      "\"\"\"\n",
      "\n",
      "------------------------------\n",
      "Cell 2:\n",
      "Cell type: code\n",
      "Source:\n",
      " import re\n",
      "\n",
      "def find_words_less_naive(word_to_find, text):\n",
      "    words = re.sub(r'[^\\w\\s]', '', text).lower().split()\n",
      "    words_not_plural = \"\".join([w[:-1] if w.endswith('s') else w for w in words])\n",
      "    matching_words = []\n",
      "    for w in words_not_plural:\n",
      "        if w == word_to_find: \n",
      "            matching_words.append(w)\n",
      "    return len(matching_words)\n",
      "\n",
      "\n",
      "word_to_find = \"fascinating\"\n",
      "count = find_words_less_naive(word_to_find, text)\n",
      "print(f'The word \"are\" appears {count} times in this text.')\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Collect a keyword from the user\n",
    "keyword = input(\"Enter a keyword to search for: \")\n",
    "\n",
    "# 2. Indicates all notebooks/cells that contain that keyword\n",
    "def search_keyword_in_notebook(keyword, notebook_json):\n",
    "    results = []\n",
    "    for cell in notebook_json['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            source = ''.join(cell['source'])\n",
    "            if keyword in source:\n",
    "                results.append({\n",
    "                    'cell_type': cell['cell_type'],\n",
    "                    'source': source,\n",
    "                    'metadata': cell.get('metadata', {})\n",
    "                })\n",
    "    return results\n",
    "\n",
    "\n",
    "# 3. Search for the keyword in the notebook\n",
    "results = search_keyword_in_notebook(keyword, notebook_classic_1_json)\n",
    "print(f\"Keyword '{keyword}' found in {len(results)} cell(s) in notebook '{notebook_classic_1}':\")\n",
    "for i, cell in enumerate(results, start=1):\n",
    "    print(\"\\n------------------------------\")\n",
    "    print(f\"Cell {i}:\")\n",
    "    print(\"Cell type:\", cell['cell_type'])\n",
    "    print(\"Source:\\n\", cell['source'])\n",
    "    if cell.get('metadata'):\n",
    "        print(\"Metadata:\", cell['metadata'])\n",
    "print(\"\\n------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77a56a",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c82055",
   "metadata": {},
   "source": [
    "There is an inherent fragility in the previous system: it requires the user to guess the keyword correctly. Trivia fact: in the early 2000s, the hability to guess keywords in Google had the same hype that using AI chatbots has nowadays.\n",
    "\n",
    "We could prevent our user from trying to guess the exact keyword or phrase, afterall, we have an estimator for phrase similarity: BERT!\n",
    "\n",
    "Improve your code so that it:\n",
    "\n",
    "1. Collects a phrase from the user,\n",
    "1. Calculates the phrase embedding $q$ using the CLS token from BERT\n",
    "1. Traverses the course material calculating the embedding $x_i$ for each cell\n",
    "1. Finds the $k$ (try with $k=1$, then generalize to any $k$) cells with minimal cosine distance ($d = \\frac{ <q, x_1>}{||x|| ||c_i||}$) with relationship to the phrase.\n",
    "\n",
    "Reflect: was this a better choice for retrieval? How can we measure this difference? (tip: research how information retrieval systems are evaluated!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7d5633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 most similar cells to the phrase 'llams':\n",
      "\n",
      "------------------------------\n",
      "Cell 1:\n",
      "Similarity: 0.8458878\n",
      "Cell type: code\n",
      "Source:\n",
      " text = \"\"\"\n",
      "Llamas are fascinating animals that are found in the Andes Mountains.\n",
      "Yoda says that cute animals they are.\n",
      "Are llamas cute? Yes they are!\n",
      "A llama never leaves its herd and will protect it with its life.\n",
      "They are known for their long necks and thick fur.\n",
      "Llamas are used as pack animals by indigenous people because they are strong and can carry heavy loads.\n",
      "They are also very social animals and are often seen in groups.\n",
      "Llamas ARE herbivores, which means they are only eating plants.\n",
      "They are also known for their gentle and calm nature, which makes them popular in petting zoos.\n",
      "Overall, llamas are remarkable creatures that are a joy to observe and are important to the cultures where they are found.\n",
      "\"\"\"\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Collects a phrase from the user\n",
    "phrase = input(\"Enter a phrase to search for: \")\n",
    "\n",
    "# 2 Calculates the phrase embedding q using the CLS token from BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(phrase, return_tensors='pt')\n",
    "\n",
    "\n",
    "# 3. Traverses the course material calculating the embedding $x_i$ for each cell\n",
    "def calculate_cell_embedding(cell_source):\n",
    "    inputs = tokenizer(cell_source, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        x_i = outputs.last_hidden_state[:, 0, :].numpy()  # CLS token embedding\n",
    "    return x_i.flatten()  # return a 1D vector\n",
    "\n",
    "\n",
    "# 4. Finds the $k$ (try with $k=1$, then generalize to any $k$) cells with minimal cosine distance ($d = \\frac{ <q, x_1>}{||x|| ||c_i||}$) with relationship to the phrase.\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def find_most_similar_cells(q, notebook_json, k=1):\n",
    "    similarities = []\n",
    "    for cell in notebook_json['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            source = ''.join(cell['source'])\n",
    "            x_i = calculate_cell_embedding(source)\n",
    "            similarity = cosine_similarity(q, x_i)\n",
    "            similarities.append((similarity, cell))\n",
    "    # Sort by similarity and get the top k\n",
    "    similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "    return similarities[:k]\n",
    "\n",
    "# 5. Prints the $k$ cells with the highest similarity\n",
    "def print_most_similar_cells(q, notebook_json, k=1):\n",
    "    most_similar_cells = find_most_similar_cells(q, notebook_json, k)\n",
    "    print(f\"Top {k} most similar cells to the phrase '{phrase}':\")\n",
    "    for i, (similarity, cell) in enumerate(most_similar_cells, start=1):\n",
    "        print(\"\\n------------------------------\")\n",
    "        print(f\"Cell {i}:\")\n",
    "        print(\"Similarity:\", similarity)\n",
    "        print(\"Cell type:\", cell['cell_type'])\n",
    "        print(\"Source:\\n\", ''.join(cell['source']))\n",
    "        if cell.get('metadata'):\n",
    "            print(\"Metadata:\", cell['metadata'])\n",
    "    print(\"\\n------------------------------\")\n",
    "print_most_similar_cells(q, notebook_classic_1_json, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6f2fd",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Now let's leave our retrival system waiting for a while.\n",
    "\n",
    "Make a small program that:\n",
    "\n",
    "1. Collects a question from the user\n",
    "1. Uses an API to redirect this question to an LLM, and immediately returns the answer.\n",
    "1. Add prompt information so that your answers can only regard NLP-related subjects (these are called \"safeguards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5aa3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM:\n",
      "{\n",
      "  \"color\": \"green\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 1. Collects a question from the user\n",
    "question = input(\"Enter a question to search for: \")\n",
    "\n",
    "# 2. Uses an API to redirect this question to an LLM, and immediately returns the answer.\n",
    "generation_config = genai.GenerationConfig(\n",
    "    max_output_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    response_mime_type='application/json'\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.0-flash\",\n",
    "                              system_instruction = \"You are an NLP expert. Please ensure that your response only pertains to topics related to Natural Language Processing (NLP).\"\n",
    "                            )\n",
    "\n",
    "try:\n",
    "    response = model.generate_content(question,generation_config=generation_config)\n",
    "    print(\"Response from LLM:\")\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while generating content:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f86f67",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Now, let's joint everything.\n",
    "\n",
    "We are able to find specific information from our courseware. Also, we are able to use LLMs. Use both abilities to:\n",
    "\n",
    "1. Collect a question from the user\n",
    "1. Retrieve the $K$ most relevant cells from the course material\n",
    "1. Use the content of these cells as part of a prompt. The prompt includes both the question and the content from the relevant cells.\n",
    "1. Phrase your prompt so that the LLM can only return information that is contained in the course material.\n",
    "\n",
    "Reflect: how does this compare to the system in Exercise 4? How can we measure the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cea7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from LLM:\n",
      "[\n",
      "  {\n",
      "    \"special_sets\": [\n",
      "      \"EMAIL_REGEX\",\n",
      "      \"HASHTAG_REGEX\",\n",
      "      \"SOCIAL_NETWORKS_REGEX\",\n",
      "      \"PHONE_NUMBERS_REGEX\",\n",
      "      \"ZIP_CODE_REGEX\"\n",
      "    ],\n",
      "    \"description\": \"The provided code uses regular expressions to find patterns in strings. The variables EMAIL_REGEX, HASHTAG_REGEX, SOCIAL_NETWORKS_REGEX, PHONE_NUMBERS_REGEX, and ZIP_CODE_REGEX are likely regular expressions designed to match specific patterns for emails, hashtags, social network mentions, phone numbers, and zip codes, respectively. These are special sets of characters or patterns used to identify these specific data types within text.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load embedding model (separate from your generative LLM)\n",
    "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embed_model_name)\n",
    "\n",
    "# 2. Compute embedding for a piece of text (question or code cell)\n",
    "def calculate_cell_embedding(text: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tokenizes the input text and returns a mean-pooled embedding tensor.\n",
    "    \"\"\"\n",
    "    inputs = embed_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = embed_model(**inputs)\n",
    "    # outputs.last_hidden_state shape: (batch_size, seq_len, hidden_size)\n",
    "    token_embeddings = outputs.last_hidden_state  # (1, seq_len, d)\n",
    "    attention_mask = inputs.attention_mask.unsqueeze(-1)  # (1, seq_len, 1)\n",
    "    summed = torch.sum(token_embeddings * attention_mask, dim=1)  # (1, d)\n",
    "    counts = torch.clamp(attention_mask.sum(dim=1), min=1e-9)  # (1, 1)\n",
    "    mean_pooled = summed / counts  # (1, d)\n",
    "    return mean_pooled[0]\n",
    "\n",
    "# 3. Find the k most similar code cells to the question\n",
    "def find_most_similar_cells(question: str, notebook_json: dict, k: int = 1):\n",
    "    \"\"\"\n",
    "    Returns a list of (similarity_score, cell) for the top-k similar code cells.\n",
    "    \"\"\"\n",
    "    q_embedding = calculate_cell_embedding(question).cpu().numpy().reshape(1, -1)\n",
    "\n",
    "    similarities = []\n",
    "    for cell in notebook_json.get(\"cells\", []):\n",
    "        if cell.get(\"cell_type\") == \"code\":\n",
    "            source = ''.join(cell.get(\"source\", []))\n",
    "            cell_emb = calculate_cell_embedding(source).cpu().numpy().reshape(1, -1)\n",
    "            sim = cosine_similarity(q_embedding, cell_emb)[0][0]\n",
    "            similarities.append((sim, cell))\n",
    "    similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "    return similarities[:k]\n",
    "\n",
    "# 4. Retrieve relevant cells\n",
    "def retrieve_relevant_cells(question: str, notebook_json: dict, k: int = 1):\n",
    "    \"\"\"Wrapper that calls find_most_similar_cells.\"\"\"\n",
    "    return find_most_similar_cells(question, notebook_json, k)\n",
    "\n",
    "def create_prompt(question: str, relevant_cells: list) -> str:\n",
    "    \"\"\"\n",
    "    Formats a prompt with the question and the top relevant cells.\n",
    "    \"\"\"\n",
    "    prompt = f\"Question: {question}\\n\\nRelevant Cells:\\n\"\n",
    "    for i, (sim, cell) in enumerate(relevant_cells, start=1):\n",
    "        prompt += f\"\\nCell {i} (Similarity: {sim:.4f}):\\n\"\n",
    "        prompt += ''.join(cell.get('source', [])) + \"\\n\"\n",
    "    return prompt\n",
    "\n",
    "def create_final_prompt(question: str, relevant_cells: list) -> str:\n",
    "    base = create_prompt(question, relevant_cells)\n",
    "    base += \"\\nPlease answer the question based only on the provided course material.\"\n",
    "    return base\n",
    "\n",
    "def answer_question_with_llm(question: str, notebook_json: dict, k: int = 1):\n",
    "    relevant_cells = retrieve_relevant_cells(question, notebook_json, k)\n",
    "    prompt = create_final_prompt(question, relevant_cells)\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        print(\"Response from LLM:\")\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while generating content:\", str(e))\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# What are some special sets we should be aware of in regex?\n",
    "question = input(\"Enter a question to search for: \")\n",
    "answer_question_with_llm(question, notebook_classic_1_json, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977a488",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "If you have reached this far, let's start optimizing our systems.\n",
    "\n",
    "To do so:\n",
    "\n",
    "1. Identify which step of your processing pipeline takes the longer\n",
    "1. Study if there are techniques or data structures that can make this specific step faster\n",
    "1. If possible, implement the optimization and test the results.\n",
    "1. Iterate until you cannot optimize anymore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
