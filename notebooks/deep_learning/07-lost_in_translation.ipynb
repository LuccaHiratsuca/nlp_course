{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5227c5",
   "metadata": {},
   "source": [
    "# ACTIVITY: LOST IN TRANSLATION\n",
    "\n",
    "**ACTIVITY DESCRIPTION: At the end of this activity, the learner will be able to compare different methodologies for classification of non-English texts**\n",
    "\n",
    "**WHAT YOU SHOULD FOCUS ON: Use and compare different methodologies for a similar task**\n",
    "\n",
    "One (very big) problem of natural language processing is that most data *corpi* are in English, but most people in the world are not English speakers. There are languages that have few speakers, or that have few available documents for training. This is especially true in the context of large-scale models, which are known for being *data hungry*, that is, they require *a lot* of data to be adequately trained.\n",
    "\n",
    "There are essentially two methods to deal with non-English texts:\n",
    "\n",
    "1. Translate the text to English and then act on the translation\n",
    "1. Train language models in different languages\n",
    "\n",
    "Nowadays, translation is relatively easy to perform using the `argostranslate` package in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4ed72a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ARGOS_DEVICE_TYPE\"] = \"cpu\" # or \"cpu\"\n",
    "\n",
    "import argostranslate.package\n",
    "import argostranslate.translate\n",
    "\n",
    "# First, we need to configure the translator:\n",
    "def _configure_argos(from_code=\"pt\", to_code=\"en\"):\n",
    "    argostranslate.package.update_package_index()\n",
    "    available_packages = argostranslate.package.get_available_packages()\n",
    "    available_package = list(\n",
    "        filter(lambda x: x.from_code == from_code and x.to_code == to_code,\n",
    "               available_packages))[0]\n",
    "    download_path = available_package.download()\n",
    "    argostranslate.package.install_from_path(download_path)\n",
    "    installed_languages = argostranslate.translate.get_installed_languages()\n",
    "    from_lang = list(filter(lambda x: x.code == from_code,\n",
    "                            installed_languages))[0]\n",
    "    to_lang = list(filter(lambda x: x.code == to_code, installed_languages))[0]\n",
    "    translation = from_lang.get_translation(to_lang)\n",
    "    return translation\n",
    "\n",
    "translator = _configure_argos('pt', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e995db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, how are you?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate(\"Olá, como você está?\")  # \"Hello, how are you?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ffcd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6b04e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27e34520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i walk this lonely road, the only one that i have ever known!\n",
      "eu caminhei sozinho pela terra, falei com as estrelas e com a lua\n"
     ]
    }
   ],
   "source": [
    "print(unmasker(\"I walk this lonely [MASK], the only one that I have ever known!\")[0]['sequence'])\n",
    "print(unmasker(\"Eu caminhei sozinho pela [MASK], falei com as estrelas e com a Lua\")[0]['sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e6fb5",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Compare three different approaches to tackle the classification problem for texts in Portuguese. For each problem, make a learning curve, that is, make random samples of the training dataset with different sizes.\n",
    "\n",
    "1. Perform classification directly in Portuguese, using a Bag-of-Words approach\n",
    "1. Classify texts directly in Portuguese using multilingual BERT\n",
    "1. Translate texts to English and then use the usual BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "819f7750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.12).\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "post_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "finbertptbr_prediction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "gpt_prediction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "real_sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6864cd45-7536-4010-a973-f564a53b3f29",
       "rows": [
        [
         "0",
         "1818246570069934451",
         "a partir desse cálculo, para cada empresa do ibovespa, selecionam-se as ações que estão entre as de 33% de menor valor, com a carteira sendo rebalanceada a cada 4 meses.",
         "2024-07-30T08:25:38.000000Z",
         "NEUTRAL",
         "NEUTRAL",
         "NEUTRAL"
        ],
        [
         "1",
         "1818251106570395982",
         "ibovespa reage às entrelinhas do relatório de produção da petrobras e aos balanços da vivo e da ccr antes da super quarta",
         "2024-07-30T08:43:39.000000Z",
         "NEUTRAL",
         "NEUTRAL",
         "NEUTRAL"
        ],
        [
         "2",
         "1818257465793765640",
         "ibovespa em queda de olho na reunião do copom",
         "2024-07-30T09:08:56.000000Z",
         "NEGATIVE",
         "NEGATIVE",
         "NEGATIVE"
        ],
        [
         "3",
         "1818262436144669087",
         "esquenta dos mercados \nbom dia!\no ibovespa abre hoje a penúltima sessão de julho com uma alta acumulada de 2,46% no mês. quase todo esse avanço, porém, ocorreu na primeira quinzena. na segunda metade de julho, o principal índice de ações da b3 recuou e…",
         "2024-07-30T09:28:41.000000Z",
         "POSITIVE",
         "NEUTRAL",
         "NEUTRAL"
        ],
        [
         "4",
         "1818262819063660642",
         "ibovespa futuro cai na expectativa pelo 1º dia de copom",
         "2024-07-30T09:30:12.000000Z",
         "NEGATIVE",
         "NEGATIVE",
         "NEGATIVE"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>finbertptbr_prediction</th>\n",
       "      <th>gpt_prediction</th>\n",
       "      <th>real_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1818246570069934451</td>\n",
       "      <td>a partir desse cálculo, para cada empresa do i...</td>\n",
       "      <td>2024-07-30T08:25:38.000000Z</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1818251106570395982</td>\n",
       "      <td>ibovespa reage às entrelinhas do relatório de ...</td>\n",
       "      <td>2024-07-30T08:43:39.000000Z</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1818257465793765640</td>\n",
       "      <td>ibovespa em queda de olho na reunião do copom</td>\n",
       "      <td>2024-07-30T09:08:56.000000Z</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1818262436144669087</td>\n",
       "      <td>esquenta dos mercados \\nbom dia!\\no ibovespa a...</td>\n",
       "      <td>2024-07-30T09:28:41.000000Z</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1818262819063660642</td>\n",
       "      <td>ibovespa futuro cai na expectativa pelo 1º dia...</td>\n",
       "      <td>2024-07-30T09:30:12.000000Z</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               post_id                                               text  \\\n",
       "0  1818246570069934451  a partir desse cálculo, para cada empresa do i...   \n",
       "1  1818251106570395982  ibovespa reage às entrelinhas do relatório de ...   \n",
       "2  1818257465793765640      ibovespa em queda de olho na reunião do copom   \n",
       "3  1818262436144669087  esquenta dos mercados \\nbom dia!\\no ibovespa a...   \n",
       "4  1818262819063660642  ibovespa futuro cai na expectativa pelo 1º dia...   \n",
       "\n",
       "                    created_at finbertptbr_prediction gpt_prediction  \\\n",
       "0  2024-07-30T08:25:38.000000Z                NEUTRAL        NEUTRAL   \n",
       "1  2024-07-30T08:43:39.000000Z                NEUTRAL        NEUTRAL   \n",
       "2  2024-07-30T09:08:56.000000Z               NEGATIVE       NEGATIVE   \n",
       "3  2024-07-30T09:28:41.000000Z               POSITIVE        NEUTRAL   \n",
       "4  2024-07-30T09:30:12.000000Z               NEGATIVE       NEGATIVE   \n",
       "\n",
       "  real_sentiment  \n",
       "0        NEUTRAL  \n",
       "1        NEUTRAL  \n",
       "2       NEGATIVE  \n",
       "3        NEUTRAL  \n",
       "4       NEGATIVE  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jvbeltra/sentiment-analysis-pt-br-stock-market-tweets\")\n",
    "path = Path(path)\n",
    "df = pd.read_csv(path / \"analise_sentimentos_ibovespa_twitter.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ca4de",
   "metadata": {},
   "source": [
    "## First Approach: Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a63e8650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.77      0.19      0.30        53\n",
      "     NEUTRAL       0.61      0.74      0.67        98\n",
      "    POSITIVE       0.71      0.83      0.76       107\n",
      "\n",
      "    accuracy                           0.67       258\n",
      "   macro avg       0.70      0.59      0.58       258\n",
      "weighted avg       0.68      0.67      0.63       258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test_bow = train_test_split(df['text'], df['real_sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_bow = pipeline.predict(X_test)\n",
    "print(classification_report(y_test_bow, y_pred_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54c745",
   "metadata": {},
   "source": [
    "## Second Approach: Multilingual BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee6d8f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1290/1290 [00:37<00:00, 34.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_embeddings(text, model, tokenizer):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[0, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "tokenizer_pt = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model_pt = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "embeddings = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    e = get_embeddings(df.iloc[i]['text'], model_pt, tokenizer_pt)\n",
    "    embeddings.append(e.detach().numpy())\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "np.save('bert_pt_embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c099d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_pt = np.load('bert_pt_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3873ffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.42      0.21      0.28        53\n",
      "     NEUTRAL       0.59      0.61      0.60        98\n",
      "    POSITIVE       0.65      0.79      0.71       107\n",
      "\n",
      "    accuracy                           0.60       258\n",
      "   macro avg       0.55      0.53      0.53       258\n",
      "weighted avg       0.58      0.60      0.58       258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_pt, df['real_sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc57c7",
   "metadata": {},
   "source": [
    "## Third Approach: Translate to English and use classic BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "660cb62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text_en'] = df['text'].apply(lambda x: translator.translate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "712d16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test_mbert = train_test_split(df['text_en'], df['real_sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25394465",
   "metadata": {},
   "source": [
    "### Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd4d4d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.56      0.17      0.26        53\n",
      "     NEUTRAL       0.60      0.74      0.66        98\n",
      "    POSITIVE       0.72      0.80      0.76       107\n",
      "\n",
      "    accuracy                           0.65       258\n",
      "   macro avg       0.63      0.57      0.56       258\n",
      "weighted avg       0.64      0.65      0.62       258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_mbert = pipeline.predict(X_test)\n",
    "print(classification_report(y_test_mbert, y_pred_mbert))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ffce7",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5a0c5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.31      0.28      0.30        53\n",
      "     NEUTRAL       0.57      0.64      0.61        98\n",
      "    POSITIVE       0.67      0.63      0.65       107\n",
      "\n",
      "    accuracy                           0.56       258\n",
      "   macro avg       0.52      0.52      0.52       258\n",
      "weighted avg       0.56      0.56      0.56       258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model     = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            outputs = self.model(**inputs)\n",
    "            cls_emb = outputs.last_hidden_state[0, 0, :].detach().numpy()\n",
    "            embeddings.append(cls_emb)\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test_enbert = train_test_split(df['text_en'], df['real_sentiment'], test_size=0.2, random_state=42)\n",
    "pipeline = Pipeline([\n",
    "    ('bert', BertTransformer()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_enbert = pipeline.predict(X_test)\n",
    "print(classification_report(y_test_enbert, y_pred_enbert))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4294fe",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "840674c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/luccahiratsuca/Github/Insper/7 Semestre/nlp_course/env/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     52\u001b[0m plot_curve(pipeline_bow,   X_bow,   y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBag-of-Words\u001b[39m\u001b[38;5;124m'\u001b[39m, ax)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mplot_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_mbert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_mbert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMultilingual BERT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m plot_curve(pipeline_enbert, X_en,   y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslate→BERT\u001b[39m\u001b[38;5;124m'\u001b[39m, ax)\n\u001b[1;32m     56\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning Curves\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 36\u001b[0m, in \u001b[0;36mplot_curve\u001b[0;34m(estimator, X, y, label, ax)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_curve\u001b[39m(estimator, X, y, label, ax):\n\u001b[0;32m---> 36\u001b[0m     train_sizes, train_scores, test_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlearning_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     train_mean \u001b[38;5;241m=\u001b[39m train_scores\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m     test_mean  \u001b[38;5;241m=\u001b[39m test_scores\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Github/Insper/7 Semestre/nlp_course/env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Github/Insper/7 Semestre/nlp_course/env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:2085\u001b[0m, in \u001b[0;36mlearning_curve\u001b[0;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params, params)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n_train_samples \u001b[38;5;129;01min\u001b[39;00m train_sizes_abs:\n\u001b[1;32m   2083\u001b[0m         train_test_proportions\u001b[38;5;241m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[0;32m-> 2085\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_test_proportions\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2103\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m   2104\u001b[0m results \u001b[38;5;241m=\u001b[39m _aggregate_score_dicts(results)\n",
      "File \u001b[0;32m~/Github/Insper/7 Semestre/nlp_course/env/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Insper/7 Semestre/nlp_course/env/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Insper/7 Semestre/nlp_course/env/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/Insper/7 Semestre/nlp_course/env/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming you already have df['text'], df['text_en'], df['real_sentiment']\n",
    "# and your translator, BertTransformer, etc.\n",
    "\n",
    "# 1) Define three pipelines\n",
    "pipeline_bow = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline_mbert = Pipeline([\n",
    "    ('embed', BertTransformer(model_name='bert-base-multilingual-uncased')),\n",
    "    ('clf',   LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline_enbert = Pipeline([\n",
    "    ('embed', BertTransformer()),  # defaults to 'bert-base-uncased'\n",
    "    ('clf',   LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "# 2) Prepare data\n",
    "X_bow   = df['text']\n",
    "X_mbert = df['text']    # multilingual BERT works on original Portuguese text\n",
    "X_en    = df['text_en'] # translated text\n",
    "y       = df['real_sentiment']\n",
    "\n",
    "# 3) Helper to plot one learning curve\n",
    "def plot_curve(estimator, X, y, label, ax):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y,\n",
    "        cv=5,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    test_mean  = test_scores.mean(axis=1)\n",
    "    ax.plot(train_sizes, train_mean, '--', label=f'{label} (train)')\n",
    "    ax.plot(train_sizes, test_mean,  '-' , label=f'{label} (cv)')\n",
    "\n",
    "# 4) Create the figure\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_curve(pipeline_bow,   X_bow,   y, 'Bag-of-Words', ax)\n",
    "plot_curve(pipeline_mbert, X_mbert, y, 'Multilingual BERT', ax)\n",
    "plot_curve(pipeline_enbert, X_en,   y, 'Translate→BERT', ax)\n",
    "\n",
    "ax.set_title('Learning Curves')\n",
    "ax.set_xlabel('Training set size')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
